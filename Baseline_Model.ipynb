{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Applications/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5270, 5)\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE DATA\n",
    "filename = \"data/Labeled_Colorado_Flu_Study_Tweets_AvI_RvN_SvO.csv\"\n",
    "coloradoData = pd.read_csv(filename, sep=\"\\t\")\n",
    "print(coloradoData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Tweet_Content</th>\n",
       "      <th>Awareness_Label</th>\n",
       "      <th>Related_Label</th>\n",
       "      <th>Self_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5926297601</td>\n",
       "      <td>don't worry it's not swine flu, i already got ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5222838706</td>\n",
       "      <td>muh. if i am getting sick and it's not swine f...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5233654812</td>\n",
       "      <td>what is up with my boy?  this morning i though...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5918860304</td>\n",
       "      <td>getting better,no more piggy flu 4 me,it was n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4631607800</td>\n",
       "      <td>@robbsterr yay for man txting you.. in other n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tweet_ID                                      Tweet_Content  \\\n",
       "0  5926297601  don't worry it's not swine flu, i already got ...   \n",
       "1  5222838706  muh. if i am getting sick and it's not swine f...   \n",
       "2  5233654812  what is up with my boy?  this morning i though...   \n",
       "3  5918860304  getting better,no more piggy flu 4 me,it was n...   \n",
       "4  4631607800  @robbsterr yay for man txting you.. in other n...   \n",
       "\n",
       "   Awareness_Label  Related_Label  Self_Label  \n",
       "0              1.0            NaN         1.0  \n",
       "1              0.0            1.0         1.0  \n",
       "2              0.0            NaN         0.0  \n",
       "3              0.0            1.0         1.0  \n",
       "4              0.0            0.0         1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coloradoData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4413, 5)\n",
      "(4413,)\n",
      "(4413,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# get all data with labels present for the column we care about (Related/NotRelated)\n",
    "coloradoVal = coloradoData.dropna(subset=[\"Related_Label\"])\n",
    "print(coloradoVal.shape)\n",
    "\n",
    "# extract X and Y as np arrays, so that we can feed them to tensors. \n",
    "X = coloradoVal[\"Tweet_Content\"]\n",
    "Y = coloradoVal[\"Related_Label\"]\n",
    "print(X.values.shape)\n",
    "print(Y.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "training label shape: (3530,)\n",
      "test label shape: (441,)\n",
      "dev label shape: (442,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train, test, dev (80%,10%,10%) (3530, 441, 442 each)\n",
    "np.random.seed(42)\n",
    "# train_data, train_labels = X[:3530], Y[:3530]\n",
    "# test_data, test_labels = X[3530:3971], Y[3530:3971]\n",
    "# dev_data, dev_labels = X[3971:], Y[3971:]\n",
    "\n",
    "train_data, test_data, dev_data = np.split(X.sample(frac=1), [int(.8*len(X)), int(.9*len(X))])\n",
    "\n",
    "train_labels = Y[train_data.index]\n",
    "test_labels = Y[test_data.index]\n",
    "dev_labels = Y[dev_data.index]\n",
    "\n",
    "# convert to numpy arrays\n",
    "train_data, test_data, dev_data, train_labels, test_labels, dev_labels = \\\n",
    "    train_data.values, test_data.values, dev_data.values, \\\n",
    "    train_labels.values, test_labels.values, dev_labels.values \n",
    "train_labels = train_labels.astype(int)\n",
    "test_labels = test_labels.astype(int)\n",
    "dev_labels = dev_labels.astype(int)\n",
    "\n",
    "print(type(train_data))\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "# print(train_labels)\n",
    "# print(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression C : {'C': 0.1} best score:  0.749008498584\n",
      "Best multinomial bayes alpha:  {'alpha': 0.5} best score:  0.750991501416\n"
     ]
    }
   ],
   "source": [
    "# Find the best parameters using CountVectorizer\n",
    "cv = CountVectorizer(analyzer='word')\n",
    "cvtrain = cv.fit_transform(train_data)\n",
    "# print(train.shape) # [samples, features] (3530, 8553)\n",
    "\n",
    "# Find the best parameters for C in logistic regression\n",
    "logit = LogisticRegression() # default penalty='l2'\n",
    "clist = {'C': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 2.0, 10.0]}\n",
    "lr = GridSearchCV(logit,clist)\n",
    "lr.fit(cvtrain, train_labels)\n",
    "print \"Best logistic regression C :\", lr.best_params_, \"best score: \",lr.best_score_\n",
    "\n",
    "# for c in clist['C']:\n",
    "#     logit2=LogisticRegression(C=c)\n",
    "#     logit2.fit(cvtrain, train_labels)\n",
    "#     weight=[]\n",
    "#     for x in range(len(logit2.coef_)):\n",
    "#         weight.append(sum(logit2.coef_[x]**2))\n",
    "#     print \"C=\", c, \", sum of squared weight values:\", weight\n",
    "    \n",
    "# Find the best parameters for alpha in Multinomial Bayes\n",
    "mb = MultinomialNB()\n",
    "alphas = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 2.0, 10.0]}        \n",
    "mnb= GridSearchCV(mb, alphas)\n",
    "mnb.fit(cvtrain, train_labels)\n",
    "print \"Best multinomial bayes alpha: \", mnb.best_params_,\"best score: \", mnb.best_score_\n",
    "\n",
    "# for a in alphas['alpha']:\n",
    "#     mb2=MultinomialNB(alpha=a)\n",
    "#     mb2.fit(cvtrain, train_labels)\n",
    "#     weight=[]\n",
    "#     for x in range(len(mb2.coef_)):\n",
    "#         weight.append(sum(mb2.coef_[x]**2))\n",
    "#     print \"alpha=\", a, \", sum of squared weight values:\", weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression C : {'C': 0.5} best score:  0.747875354108\n",
      "Best multinomial bayes alpha:  {'alpha': 0.3} best score:  0.738526912181\n"
     ]
    }
   ],
   "source": [
    "# Find the best parameters using TfidfVectorizer\n",
    "tf= TfidfVectorizer(analyzer='word')\n",
    "tftrain=tf.fit_transform(train_data)\n",
    "\n",
    "# Find the best parameters for C in logistic regression\n",
    "logit = LogisticRegression() # default penalty='l2'\n",
    "clist = {'C': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 2.0, 10.0]}\n",
    "lr = GridSearchCV(logit,clist)\n",
    "lr.fit(tftrain, train_labels)\n",
    "print \"Best logistic regression C :\", lr.best_params_, \"best score: \",lr.best_score_\n",
    "\n",
    "# for c in clist['C']:\n",
    "#     logit2=LogisticRegression(C=c)\n",
    "#     logit2.fit(train, train_labels)\n",
    "#     weight=[]\n",
    "#     for x in range(len(logit2.coef_)):\n",
    "#         weight.append(sum(logit2.coef_[x]**2))\n",
    "#     print \"C=\", c, \", sum of squared weight values:\", weight\n",
    "    \n",
    "# Find the best parameters for alpha in Multinomial Bayes\n",
    "mb = MultinomialNB()\n",
    "alphas = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0, 2.0, 10.0]}        \n",
    "mnb= GridSearchCV(mb, alphas)\n",
    "mnb.fit(tftrain, train_labels)\n",
    "print \"Best multinomial bayes alpha: \", mnb.best_params_,\"best score: \", mnb.best_score_\n",
    "\n",
    "# for a in alphas['alpha']:\n",
    "#     mb2=MultinomialNB(alpha=a)\n",
    "#     mb2.fit(tftrain, train_labels)\n",
    "#     weight=[]\n",
    "#     for x in range(len(mb2.coef_)):\n",
    "#         weight.append(sum(mb2.coef_[x]**2))\n",
    "#     print \"alpha=\", a, \", sum of squared weight values:\", weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size without preprocessing:  8553\n",
      "F1 score without preprocessing:  0.739819004525\n",
      "Dictionary size with preprocessing:  8487\n",
      "F1 score with preprocessing:  0.733031674208\n",
      "Dictionary size reduction:  66\n"
     ]
    }
   ],
   "source": [
    "def better_preprocessor(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub('^[^a-zA-z]*|[^a-zA-Z]*$','',s)\n",
    "    s = re.sub('\\s+', ' ', s).strip() \n",
    "    s = re.sub(r'\\b\\d+\\b', '', s)\n",
    "    s = re.sub(r'<.*?>', '', s)\n",
    "    s = re.sub(r\"\\\\\", \"\", s)    \n",
    "    s = re.sub(r\"\\'\", \"\", s)    \n",
    "    s = re.sub(r\"\\\"\", \"\", s) \n",
    "    return s\n",
    "# TO DO: add specific twitter preprocessor\n",
    "# https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/\n",
    "\n",
    "def preprocess():\n",
    "    # no processing\n",
    "    vect = CountVectorizer(preprocessor=None) # set preprocessor to default none\n",
    "    cvdata=vect.fit_transform(train_data)\n",
    "    logit = LogisticRegression() # default penalty='l2'\n",
    "    logit.fit(cvdata, train_labels)\n",
    "    \n",
    "    dev=vect.transform(dev_data)\n",
    "    pred = logit.predict(dev)\n",
    "    score = metrics.f1_score(dev_labels, pred, average='micro')\n",
    "    print \"Dictionary size without preprocessing: \", len(vect.vocabulary_) # without preprocessing\n",
    "    print \"F1 score without preprocessing: \", score\n",
    "    \n",
    "    # preprocessing\n",
    "    cv = CountVectorizer(preprocessor=better_preprocessor)\n",
    "    cvdata2=cv.fit_transform(train_data)\n",
    "    logit2 = LogisticRegression() # default penalty='l2'\n",
    "    logit2.fit(cvdata2, train_labels)\n",
    "    \n",
    "    dev2=cv.transform(dev_data)\n",
    "    pred2 = logit2.predict(dev2)\n",
    "    score2 = metrics.f1_score(dev_labels, pred2, average='micro')\n",
    "    print \"Dictionary size with preprocessing: \", len(cv.vocabulary_)\n",
    "    print \"F1 score with preprocessing: \", score2\n",
    "    print \"Dictionary size reduction: \", len(vect.vocabulary_)-len(cv.vocabulary_)\n",
    "\n",
    "\n",
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev data processed accuracy: 0.762443438914\n",
      "Dev data processed entropy loss: 0.498619875944\n",
      "Test data processed accuracy: 0.718820861678\n",
      "Test data processed entropy loss: 0.553194566619\n"
     ]
    }
   ],
   "source": [
    "def cv_log(param, data, labels, processor=None):\n",
    "\n",
    "    tf= CountVectorizer(analyzer='word', preprocessor=processor)\n",
    "    tfdata=tf.fit_transform(train_data)\n",
    "    logit = LogisticRegression(C=param)\n",
    "    logit.fit(tfdata, train_labels)\n",
    "    \n",
    "    tfdev=tf.transform(data)\n",
    "    \n",
    "    # predict classification\n",
    "    predict= logit.predict(tfdev)\n",
    "    post_prob = logit.predict_proba(tfdev)\n",
    "    \n",
    "    accuracy=metrics.f1_score(labels, predict, average='micro')\n",
    "    loss=metrics.log_loss(labels, post_prob)\n",
    "    return [accuracy, loss]\n",
    "\n",
    "# ## STUDENT END ###\n",
    "\n",
    "# print'Dev data accuracy:', cv_log(0.1, dev_data, dev_labels)[0]\n",
    "print'Dev data processed accuracy:', cv_log(0.1, dev_data, dev_labels, better_preprocessor)[0]\n",
    "\n",
    "# print'Dev entropy loss:', cv_log(0.1, dev_data, dev_labels)[1]\n",
    "print'Dev data processed entropy loss:', cv_log(0.1, dev_data, dev_labels, better_preprocessor)[1]\n",
    "\n",
    "# print'Test data accuracy:', cv_log(0.1, test_data, test_labels)[0]\n",
    "print'Test data processed accuracy:', cv_log(0.1, test_data, test_labels,better_preprocessor)[0]\n",
    "\n",
    "# print'Test entropy loss:', cv_log(0.1, test_data, test_labels)[1]\n",
    "print'Test data processed entropy loss:', cv_log(0.1, test_data, test_labels,better_preprocessor)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev data processed accuracy: 0.755656108597\n",
      "Dev data processed entropy loss: 0.509039494116\n",
      "test data processed accuracy: 0.721088435374\n",
      "Test data processed entropy loss: 0.5537123254\n"
     ]
    }
   ],
   "source": [
    "def tfid_log(param, data, labels, processor=None):\n",
    "\n",
    "    tf= TfidfVectorizer(analyzer='word', preprocessor=processor)\n",
    "    tfdata=tf.fit_transform(train_data)\n",
    "    logit = LogisticRegression(C=param)\n",
    "    logit.fit(tfdata, train_labels)\n",
    "    \n",
    "    tfdev=tf.transform(data)\n",
    "    \n",
    "    # predict classification\n",
    "    predict= logit.predict(tfdev)\n",
    "    post_prob = logit.predict_proba(tfdev)\n",
    "    \n",
    "    accuracy=metrics.f1_score(labels, predict, average='micro')\n",
    "    loss=metrics.log_loss(labels, post_prob)\n",
    "    return [accuracy, loss]\n",
    "\n",
    "\n",
    "# print'dev data accuracy:', tfid_log(0.5, dev_data, dev_labels)[0]\n",
    "print'dev data processed accuracy:', tfid_log(0.5, dev_data, dev_labels, better_preprocessor)[0]\n",
    "\n",
    "# print'Dev entropy loss:', tfid_log(0.5, dev_data, dev_labels)[1]\n",
    "print'Dev data processed entropy loss:', tfid_log(0.5, dev_data, dev_labels, better_preprocessor)[1]\n",
    "\n",
    "# print'test data accuracy:', tfid_log(0.5, test_data, test_labels)[0]\n",
    "print'test data processed accuracy:', tfid_log(0.5, test_data, test_labels, better_preprocessor)[0]\n",
    "\n",
    "# print'Test entropy loss:', tfid_log(0.5, test_data, test_labels)[1]\n",
    "print'Test data processed entropy loss:', tfid_log(0.5, test_data, test_labels, better_preprocessor)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev data processed accuracy: 0.764705882353\n",
      "Dev data processed entropy loss: 1.20977422236\n",
      "Test data processed accuracy: 0.732426303855\n",
      "Test data processed entropy loss: 1.36136799263\n"
     ]
    }
   ],
   "source": [
    "def cv_mnb(param, data, labels, processor=None):\n",
    "\n",
    "    tf= CountVectorizer(analyzer='word', preprocessor=processor)\n",
    "    tfdata=tf.fit_transform(train_data)\n",
    "    mb = MultinomialNB(alpha=param)\n",
    "    mb.fit(tfdata, train_labels)\n",
    "    \n",
    "    tfdev=tf.transform(data)\n",
    "    \n",
    "    # predict classification\n",
    "    predict= mb.predict(tfdev)\n",
    "    post_prob = mb.predict_proba(tfdev)\n",
    "    \n",
    "    accuracy=metrics.f1_score(labels, predict, average='micro')\n",
    "    loss=metrics.log_loss(labels, post_prob)\n",
    "    return [accuracy, loss]\n",
    "\n",
    "# print'dev data accuracy:', cv_mnb(0.5, dev_data, dev_labels)[0]\n",
    "print'Dev data processed accuracy:', cv_mnb(0.5, dev_data, dev_labels, better_preprocessor)[0]\n",
    "\n",
    "# print'Dev entropy loss:', cv_mnb(0.5, dev_data, dev_labels)[1]\n",
    "print'Dev data processed entropy loss:', cv_mnb(0.5, dev_data, dev_labels, better_preprocessor)[1]\n",
    "\n",
    "# print'test data accuracy:', cv_mnb(0.5, test_data, test_labels)[0]\n",
    "print'Test data processed accuracy:', cv_mnb(0.5, test_data, test_labels, better_preprocessor)[0]\n",
    "\n",
    "# print'Test entropy loss:', cv_mnb(0.5, test_data, test_labels)[1]\n",
    "print'Test data processed entropy loss:', cv_mnb(0.5, test_data, test_labels, better_preprocessor)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev data processed accuracy: 0.755656108597\n",
      "Dev data processed entropy loss: 0.537005501462\n",
      "Test data processed accuracy: 0.716553287982\n",
      "Test data processed entropy loss: 0.592988489634\n"
     ]
    }
   ],
   "source": [
    "def tf_mnb(param, data, labels, processor=None):\n",
    "\n",
    "    tf= TfidfVectorizer(analyzer='word', preprocessor=processor)\n",
    "    tfdata=tf.fit_transform(train_data)\n",
    "    mb = MultinomialNB(alpha=param)\n",
    "    mb.fit(tfdata, train_labels)\n",
    "    \n",
    "    tfdev=tf.transform(data)\n",
    "    \n",
    "    # predict classification\n",
    "    predict= mb.predict(tfdev)\n",
    "    post_prob = mb.predict_proba(tfdev)\n",
    "    \n",
    "    accuracy=metrics.f1_score(labels, predict, average='micro')\n",
    "    loss=metrics.log_loss(labels, post_prob)\n",
    "    \n",
    "    return [accuracy, loss]\n",
    "\n",
    "\n",
    "# print'dev data accuracy:', tf_mnb(0.3, dev_data, dev_labels)[0]\n",
    "print'Dev data processed accuracy:', tf_mnb(0.3, dev_data, dev_labels, better_preprocessor)[0]\n",
    "\n",
    "# print'Dev entropy loss:', tf_mnb(0.3, dev_data, dev_labels)[1]\n",
    "print'Dev data processed entropy loss:', tf_mnb(0.3, dev_data, dev_labels, better_preprocessor)[1]\n",
    "\n",
    "# print'test data accuracy:', tf_mnb(0.3, test_data, test_labels)[0]\n",
    "print'Test data processed accuracy:', tf_mnb(0.3, test_data, test_labels, better_preprocessor)[0]\n",
    "\n",
    "# print'Test entropy loss:', tf_mnb(0.3, test_data, test_labels)[1]\n",
    "print'Test data processed entropy loss:', tf_mnb(0.3, test_data, test_labels, better_preprocessor)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
